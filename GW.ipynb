{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a581b0-c068-41fb-867e-10011c69e06d",
   "metadata": {},
   "source": [
    "# INLP Group Work HS2022\n",
    "\n",
    "This document describes the group work aspect of the INLP module. We provide descriptions and code samples of the planned tasks.\n",
    "\n",
    "Please fill in details where explicitly indicated and leave everything else intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ffa6d2-5a7d-4895-a8cb-30096eb93936",
   "metadata": {},
   "source": [
    "Group ID: 6\n",
    "\n",
    "Group members: Alina Meyer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b53fa5b-d344-48f1-b80d-219802844516",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "| Name            | Split (Train/Test/Validation) |       Type |\n",
    "|-----------------|:-----------------------------:|-----------:|\n",
    "| emotion         | 3257/1421/374                 | Quaternary |\n",
    "| hate            | 9000/2970/1000                |     Binary |\n",
    "| irony           | 2862/784/955                  |     Binary |\n",
    "| offensive       | 11916/860/1324                |     Binary |\n",
    "| sentiment       | 45615/12284/2000              |    Ternary |\n",
    "| stance_abortion | 587/280/66                    |    Ternary |\n",
    "| stance_atheism  | 461/220/52                    |    Ternary |\n",
    "| stance_climate  | 355/169/40                    |    Ternary |\n",
    "| stance_feminism | 597/285/67                    |    Ternary |\n",
    "| stance_hillary  | 620/295/69                    |    Ternary |\n",
    "\n",
    "Following the same approaches presented in the module, solve the current tasks:\n",
    "\n",
    "1. Envision an NLP application using one or more of the data sets described in the table above\n",
    "2. Implement your solution in a Jupyter Notebook\n",
    "3. Document it using the provided Canvas document (that will guide you in the required aspects)\n",
    "4. Present your solution in the final lecture of the module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d278d13f-0c2a-4d9d-b548-760f70e96456",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21e012c-6461-41f5-b750-4537751455d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-4.1.1-py3-none-any.whl (503 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m503.6/503.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "Collecting tqdm>=4.66.3\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from datasets) (25.0)\n",
      "Collecting huggingface-hub>=0.24.0\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.32.2 in ./.venv/lib/python3.10/site-packages (from datasets) (2.32.5)\n",
      "Collecting numpy>=1.17\n",
      "  Using cached numpy-2.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Collecting pyarrow>=21.0.0\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.4.1,>=0.3.0\n",
      "  Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from datasets) (6.0.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.2/193.2 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2025.9.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.13.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hf-xet<2.0.0,>=1.1.3\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2025.10.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.0/347.0 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.7/241.7 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting aiosignal>=1.4.0\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.9/196.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.5/219.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, pyarrow, propcache, numpy, multidict, hf-xet, fsspec, frozenlist, filelock, dill, async-timeout, aiohappyeyeballs, yarl, pandas, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.0 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.1.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.9.0 hf-xet-1.1.10 huggingface-hub-0.35.3 multidict-6.7.0 multiprocess-0.70.16 numpy-2.2.6 pandas-2.3.3 propcache-0.4.1 pyarrow-21.0.0 pytz-2025.2 tqdm-4.67.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e563a0-da59-450e-8565-fec2bd7330d4",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "This section provides starter code for loading all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0988825e-72d0-4113-a903-a98b79ca99c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "# NOTE: this block loads all the available data into a dictionary\n",
    "# use the keys of the dictionary to access the required data set\n",
    "all_data = {}\n",
    "names = [\"emotion\", \"hate\", \"irony\",\n",
    "         \"offensive\", \"sentiment\", \"stance_abortion\",\n",
    "         \"stance_atheism\", \"stance_climate\", \"stance_feminist\",\n",
    "         \"stance_hillary\"]\n",
    "for name in names:\n",
    "    all_data[name] = datasets.load_dataset(\"tweet_eval\", name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9efeddd1-1f23-4945-afcd-cf94673a4177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['emotion', 'hate', 'irony', 'offensive', 'sentiment', 'stance_abortion', 'stance_atheism', 'stance_climate', 'stance_feminist', 'stance_hillary'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e484273b-0a37-44db-9bc2-4402b4ef5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# print description of the \"offensive\" data set\n",
    "print(all_data[\"offensive\"][\"train\"].info.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b93b5998-119e-48bc-80b7-8d0bbceb9f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['non-offensive', 'offensive']\n"
     ]
    }
   ],
   "source": [
    "# print labels available for \"offensive\" data set (with order)\n",
    "print(all_data[\"offensive\"][\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e446c9-f93e-4f77-9a53-54c3c2d9524c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anger', 'joy', 'optimism', 'sadness']\n"
     ]
    }
   ],
   "source": [
    "print(all_data[\"emotion\"][\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a68c81fa-bd5e-4e80-9869-eb5d87819ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['none', 'against', 'favor']\n"
     ]
    }
   ],
   "source": [
    "print(all_data[\"stance_feminist\"][\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "856e0e20-cabd-4bf6-ba69-705610f84831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'neutral', 'positive']\n"
     ]
    }
   ],
   "source": [
    "print(all_data[\"sentiment\"][\"train\"].info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba936a2-dbb9-4720-abb6-c2c2d28758ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Bono... who cares. Soon people will understand that they gain nothing from following a phony celebrity. Become a Leader of your people instead or help and support your fellow countrymen.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of a non-offensive tweet\n",
    "all_data[\"offensive\"][\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eee3bae6-0adc-497b-9463-15dd6c492dd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '@user Eight years the republicans denied obama’s picks. Breitbarters outrage is as phony as their fake president.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of an offensive tweet\n",
    "all_data[\"offensive\"][\"train\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffda9ee8-15b4-4180-81d0-a9f729b31820",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "This section describes next steps in your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ccb7b4-8906-439f-9f72-477506d0d621",
   "metadata": {},
   "source": [
    "\n",
    "### Feature extraction/transformation and tokenization\n",
    "\n",
    "**Fill in** your NLP pipeline in the next blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9980ae6d-d887-4ba9-9480-186c33692c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./.venv/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./.venv/lib/python3.10/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: joblib in ./.venv/lib/python3.10/site-packages (from nltk) (1.5.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "defda669-7afd-401f-acbd-88d2528dfe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "feminist_stance_train = all_data[\"stance_feminist\"][\"train\"]\n",
    "feminist_stance_test = all_data[\"stance_feminist\"][\"test\"]\n",
    "feminist_stance_val = all_data[\"stance_feminist\"][\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c20a9d6-4b7d-4051-9141-34cff36c0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_train = all_data[\"offensive\"][\"train\"]\n",
    "offensive_test = all_data[\"offensive\"][\"test\"]\n",
    "offensive_val = all_data[\"offensive\"][\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20a0b92a-f203-4abd-b7c2-63557c59016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'RT @user Look for our latest indiegogo campaign coming out soon to help turn young girls into great leaders. #womensrights #SemST',\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feminist_stance_train[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ecd548e-4d07-4968-90ab-6caae50e6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "PUNCTUATION_AND_DIGITS = string.punctuation + string.digits\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_and_clean_text(example):\n",
    "    text = example['text'] \n",
    "    # removes unnecessary \"RT @user\"\n",
    "    text = re.sub(r'\\brt\\s?@?\\w*\\s*', '', text, flags=re.IGNORECASE)\n",
    "    # remove urls\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    tokens = tweet_tokenizer.tokenize(text)\n",
    "\n",
    "    # keeps: '#something' '@user'\n",
    "    # removes: '#' '123' '.' '@'\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if not all(c in PUNCTUATION_AND_DIGITS for c in token)\n",
    "    ]\n",
    "    \n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    bigrams = list(ngrams(lemmatized_tokens, n=2))\n",
    "    bigram_strings = ['_'.join(gram) for gram in bigrams]\n",
    "    processed_tokens = lemmatized_tokens + bigram_strings\n",
    "    \n",
    "    processed_string = \" \".join(processed_tokens)\n",
    "    return {'processed_text': processed_string}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fecfbdff-c964-4c06-8349-71c59fb74504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebe45bbad144159aa663d2edc21f7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/597 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e787be44cd57456998161aeb6f61759f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/67 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51358c93992b41b2afb23e9b708be96a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_feminist_stance_train = feminist_stance_train.map(preprocess_and_clean_text)\n",
    "processed_feminist_stance_validation = feminist_stance_val.map(preprocess_and_clean_text)\n",
    "processed_feminist_stance_test = feminist_stance_test.map(preprocess_and_clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6941ea48-4212-46be-9d3b-a3a54f7145d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead0d3df3e314654942b0c4a1a8bd02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11916 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca6a2c90f8b448c9ff305fcba7bd2e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4049eecb74d462ab86bfe0a7d003a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/860 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processed_offensive_train = offensive_train.map(preprocess_and_clean_text)\n",
    "processed_offensive_validation = offensive_val.map(preprocess_and_clean_text)\n",
    "processed_offensive_test = offensive_test.map(preprocess_and_clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692093e-cdf0-458b-9087-93a5e746968e",
   "metadata": {},
   "source": [
    "### Vocabulary and vector representation\n",
    "\n",
    "**Fill in** the code for providing the vector representation of your data set(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c06a947-cd73-4b68-87ce-9b8bba4406d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312ce2f-6625-4553-afc6-4fe2185ed387",
   "metadata": {},
   "source": [
    "### Evaluation (traditional ML)\n",
    "\n",
    "**Fill in** the code for evaluating your NLP pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02ff1ea3-640b-4b03-8710-1b5c2b076cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f2cf20-73c9-4045-9283-d3e64bbf408d",
   "metadata": {},
   "source": [
    "### Evaluation (neural network)\n",
    "\n",
    "**Fill in** the code for evaluating/comparing with a neural network (transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce02113-f3a3-4909-81c6-ba59ee68db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbb31d-50f4-4ad0-af8f-1857ef1121af",
   "metadata": {},
   "source": [
    "## That's all folks :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
